{
    "general_rule": "Rule: Optimize Input Processing for Large Data Streams\n\n1. Type of Improvement:\n   Enhance the handling of large input streams by processing data in chunks rather than reading the entire stream at once.\n\n2. Benefits:\n   - Reduces memory usage by avoiding the need to load the entire input into memory\n   - Improves performance for large inputs by allowing processing to begin before the entire input is read\n   - Prevents potential out-of-memory errors when dealing with extremely large inputs\n   - Enables setting a maximum limit on the amount of data processed, improving security and resource management\n\n3. Identifying Opportunities:\n   Look for code that:\n   - Reads entire input streams or files into memory before processing\n   - Uses methods like toString() or similar functions that consume an entire input\n   - Lacks checks for input size or doesn't impose limits on the amount of data processed\n   - Deals with potentially large or unknown-sized inputs, especially in network communications or file processing\n\n4. General Application:\n   - Implement buffered reading mechanisms to process input in manageable chunks\n   - Add size checks and limits to prevent processing of excessively large inputs\n   - Use streaming APIs or iterators when available to process data incrementally\n   - Consider implementing backpressure mechanisms in scenarios involving continuous data streams\n   - Balance chunk size to optimize between memory usage and processing efficiency\n   - Ensure error handling and resource cleanup are properly managed when processing is interrupted"
}